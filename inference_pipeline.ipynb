{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f5bcf7fb-5c3a-4eaa-be70-78626b6b4930",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python3\n",
    "# reuse_ddos_pipeline.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "MODEL_PATH = \"/home/jovyan/work/pipeline_output/ddos_xgb_pipeline\"\n",
    "METADATA_PATH = \"/home/jovyan/work/pipeline_output/pipeline_metadata.json\"\n",
    "NEW_DATA_PATH = \"/home/jovyan/work/new_data.csv\"\n",
    "\n",
    "OUTPUT_PRED_PATH = \"/home/jovyan/work/pipeline_output/new_predictions.csv\"\n",
    "\n",
    "# ============================================================\n",
    "# SPARK SESSION\n",
    "# ============================================================\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"DDoS_Pipeline_Inference\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD MODEL + METADATA\n",
    "# ============================================================\n",
    "pipeline_model = PipelineModel.load(MODEL_PATH)\n",
    "\n",
    "with open(METADATA_PATH, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "feature_cols = metadata[\"feature_columns\"]\n",
    "col_name_map = metadata[\"canonical_column_map\"]\n",
    "labels_list = metadata[\"label_mapping\"]\n",
    "\n",
    "# ============================================================\n",
    "# LOAD NEW DATA\n",
    "# ============================================================\n",
    "df_new = spark.read.csv(NEW_DATA_PATH, header=True, inferSchema=True)\n",
    "\n",
    "# Normalize column names\n",
    "for src, canon in col_name_map.items():\n",
    "    if src in df_new.columns and src != canon:\n",
    "        df_new = df_new.withColumnRenamed(src, canon)\n",
    "\n",
    "# Add missing columns\n",
    "for col in feature_cols:\n",
    "    if col not in df_new.columns:\n",
    "        df_new = df_new.withColumn(col, F.lit(0))\n",
    "\n",
    "# Convert to double\n",
    "for c in feature_cols:\n",
    "    df_new = df_new.withColumn(c, F.col(c).cast(DoubleType()))\n",
    "\n",
    "df_new = df_new.fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# RUN PIPELINE\n",
    "# ============================================================\n",
    "preds = pipeline_model.transform(df_new)\n",
    "\n",
    "# ============================================================\n",
    "# DECODE prediction → original label\n",
    "# ============================================================\n",
    "index_to_string = IndexToString(\n",
    "    inputCol=\"prediction\",\n",
    "    outputCol=\"prediction_label\",\n",
    "    labels=labels_list\n",
    ")\n",
    "preds = index_to_string.transform(preds)\n",
    "\n",
    "# ============================================================\n",
    "# ADD row_id (for merging later)\n",
    "# ============================================================\n",
    "preds = preds.withColumn(\"_tmp_id\", F.monotonically_increasing_id())\n",
    "w = Window.orderBy(\"_tmp_id\")\n",
    "preds = preds.withColumn(\"row_id\", F.row_number().over(w)).drop(\"_tmp_id\")\n",
    "\n",
    "# ============================================================\n",
    "# SELECT FINAL OUTPUT\n",
    "# ============================================================\n",
    "final_df = preds.select(\n",
    "    \"row_id\",\n",
    "    \"prediction_label\",\n",
    "    \"probability\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================\n",
    "final_df.toPandas().to_csv(OUTPUT_PRED_PATH, index=False)\n",
    "print(f\"✅ Predictions saved to: {OUTPUT_PRED_PATH}\")\n",
    "\n",
    "print(\"===== SAMPLE OUTPUT =====\")\n",
    "final_df.show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
